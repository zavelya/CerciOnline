{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TaIO5DNlaEMb"
      },
      "outputs": [],
      "source": [
        "import torch as tch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, precision_recall_curve\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi_rajCpaMB8",
        "outputId": "441dfa0f-1073-4df0-e1fc-b169ceb90e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = tch.device(\"cuda\" if tch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "K1rWi0VtauHt"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# LSTM Model\n",
        "# -------------------------------\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.3)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = tch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = tch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Focal Loss for imbalanced data\n",
        "# -------------------------------\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "        probs = tch.sigmoid(inputs)\n",
        "        pt = tch.where(targets == 1, probs, 1 - probs)\n",
        "        loss = self.alpha * (1 - pt) ** self.gamma * BCE\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Sequence creation\n",
        "# -------------------------------\n",
        "def create_sequences(data, labels, seq_len, step=10):\n",
        "    X, y = [], []\n",
        "    for i in range(0, len(data) - seq_len, step):  # step instead of 1\n",
        "        X.append(data[i:i+seq_len])\n",
        "        y.append(labels[i+seq_len])\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HY_5JkNrfnhU"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Dataset class\n",
        "# -------------------------------\n",
        "class ESADataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = tch.tensor(self.data[idx]).float()\n",
        "        y = tch.tensor(self.labels[idx]).float().unsqueeze(-1)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EW7bdnHeazco"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Train model\n",
        "# -------------------------------\n",
        "def train_model(model, optimizer, scheduler, criterion, start_epoch, num_epoch, train_loader):\n",
        "    for epoch in range(start_epoch, num_epoch):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        grad_norm_sum = 0.0\n",
        "        grad_batches = 0\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            tch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_norm = 0.0\n",
        "            for p in model.parameters():\n",
        "                if p.grad is not None:\n",
        "                    param_norm = p.grad.data.norm(2)\n",
        "                    total_norm += param_norm.item() ** 2\n",
        "            total_norm = total_norm ** 0.5\n",
        "            grad_norm_sum += total_norm\n",
        "            grad_batches += 1\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        avg_grad_norm = grad_norm_sum / grad_batches\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epoch}, Loss: {avg_loss:.4f}, Avg Grad Norm: {avg_grad_norm:.4f}, LR: {lr:.6f}\")\n",
        "\n",
        "        if isinstance(scheduler, tch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "            scheduler.step(avg_loss)\n",
        "        else:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Save checkpoint\n",
        "        tch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'loss': avg_loss,\n",
        "        }, f'models/Epoch_save/checkpoint_epoch{epoch}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "collapsed": true,
        "id": "4aAl1lI5a9zi"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -------------------------------\n",
        "# DATA PREPARATION\n",
        "# -------------------------------\n",
        "dataset = pd.read_csv(\"data/sample_dataset.csv\")\n",
        "dataset['datetime'] = pd.to_datetime(dataset['datetime'], utc=True)\n",
        "dataset = dataset.set_index('datetime')\n",
        "\n",
        "features = dataset.drop(columns=['label', 'Tc_lvl2', 'Tc_lvl3'])\n",
        "labels = dataset['label']\n",
        "\n",
        "# Resample to 4 minutes\n",
        "features_resampled = features.resample('240s').ffill()\n",
        "labels_resampled = labels.resample('240s').max().reindex(features_resampled.index)\n",
        "\n",
        "resampled_df = features_resampled.join(labels_resampled)\n",
        "resampled_df.fillna(0, inplace=True)\n",
        "\n",
        "# Feature Engineering\n",
        "for col in features_resampled.columns:\n",
        "    resampled_df[f\"{col}_diff\"] = resampled_df[col].diff()\n",
        "    resampled_df[f\"{col}_std\"] = resampled_df[col].rolling(30).std()\n",
        "\n",
        "resampled_df.fillna(0, inplace=True)\n",
        "\n",
        "# Prepare arrays\n",
        "X_all = resampled_df.drop(columns=[\"label\"]).to_numpy()\n",
        "y_all = resampled_df[\"label\"].to_numpy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.3, random_state=42, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbxtvyBQbB-t",
        "outputId": "558e8803-d893-4721-b856-b93ede986e74"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['models/Scalar_save/scaler.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "#Scaling dataset\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "joblib.dump(scaler, 'models/Scalar_save/scaler.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec9074a4",
        "outputId": "308227d8-ce6f-42f5-c0ac-1cf6df84788e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7JCMZSY5bWm8"
      },
      "outputs": [],
      "source": [
        "# Sequence creation\n",
        "seq_length = 120\n",
        "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, seq_length)\n",
        "\n",
        "# Compute weights for sampler\n",
        "y_train_seq_t = tch.from_numpy(y_train_seq).float()\n",
        "class_sample_count = np.array([(y_train_seq_t == 0).sum().item(), (y_train_seq_t == 1).sum().item()])\n",
        "weights = 1. / class_sample_count\n",
        "samples_weight = np.array([weights[int(t)] for t in y_train_seq])\n",
        "samples_weight = tch.from_numpy(samples_weight)\n",
        "sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight), replacement=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RQfiGEX6bmLr"
      },
      "outputs": [],
      "source": [
        "# Dataset + Dataloader\n",
        "train_dataset = ESADataset(X_train_seq, y_train_seq)\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, sampler=sampler, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "du1MmmnJbxG1"
      },
      "outputs": [],
      "source": [
        "#Model parameters\n",
        "n_features = X_train_seq.shape[2]\n",
        "hidden_size = 100\n",
        "num_layers = 3\n",
        "num_epoch = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DQ5aMj3DcEPu"
      },
      "outputs": [],
      "source": [
        "# Model creation with optimizer and learning scheduler\n",
        "model = NeuralNet(n_features, hidden_size, num_layers).to(device)\n",
        "optimizer = tch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = tch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "criterion = FocalLoss(alpha=0.25, gamma=2.0).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKZT5WPfcT1u",
        "outputId": "b91cea9c-852c-4852-d3c3-e881e210e8e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 0.0432, Avg Grad Norm: 0.0073, LR: 0.000100\n",
            "Epoch 2/50, Loss: 0.0432, Avg Grad Norm: 0.0071, LR: 0.000100\n",
            "Epoch 3/50, Loss: 0.0433, Avg Grad Norm: 0.0014, LR: 0.000100\n",
            "Epoch 4/50, Loss: 0.0430, Avg Grad Norm: 0.0131, LR: 0.000100\n",
            "Epoch 5/50, Loss: 0.0435, Avg Grad Norm: 0.0117, LR: 0.000100\n",
            "Epoch 6/50, Loss: 0.0432, Avg Grad Norm: 0.0038, LR: 0.000100\n",
            "Epoch 7/50, Loss: 0.0433, Avg Grad Norm: 0.0011, LR: 0.000100\n",
            "Epoch 8/50, Loss: 0.0437, Avg Grad Norm: 0.0211, LR: 0.000100\n",
            "Epoch 9/50, Loss: 0.0429, Avg Grad Norm: 0.0191, LR: 0.000010\n",
            "Epoch 10/50, Loss: 0.0430, Avg Grad Norm: 0.0191, LR: 0.000010\n",
            "Epoch 11/50, Loss: 0.0433, Avg Grad Norm: 0.0026, LR: 0.000010\n",
            "Epoch 12/50, Loss: 0.0433, Avg Grad Norm: 0.0027, LR: 0.000010\n",
            "Epoch 13/50, Loss: 0.0433, Avg Grad Norm: 0.0011, LR: 0.000010\n",
            "Epoch 14/50, Loss: 0.0434, Avg Grad Norm: 0.0086, LR: 0.000001\n",
            "Epoch 15/50, Loss: 0.0431, Avg Grad Norm: 0.0129, LR: 0.000001\n",
            "Epoch 16/50, Loss: 0.0432, Avg Grad Norm: 0.0038, LR: 0.000001\n",
            "Epoch 17/50, Loss: 0.0433, Avg Grad Norm: 0.0027, LR: 0.000001\n",
            "Epoch 18/50, Loss: 0.0434, Avg Grad Norm: 0.0057, LR: 0.000000\n",
            "Epoch 19/50, Loss: 0.0434, Avg Grad Norm: 0.0027, LR: 0.000000\n",
            "Epoch 20/50, Loss: 0.0433, Avg Grad Norm: 0.0013, LR: 0.000000\n",
            "Epoch 21/50, Loss: 0.0432, Avg Grad Norm: 0.0067, LR: 0.000000\n",
            "Epoch 22/50, Loss: 0.0432, Avg Grad Norm: 0.0067, LR: 0.000000\n",
            "Epoch 23/50, Loss: 0.0434, Avg Grad Norm: 0.0087, LR: 0.000000\n",
            "Epoch 24/50, Loss: 0.0434, Avg Grad Norm: 0.0012, LR: 0.000000\n",
            "Epoch 25/50, Loss: 0.0436, Avg Grad Norm: 0.0178, LR: 0.000000\n",
            "Epoch 26/50, Loss: 0.0433, Avg Grad Norm: 0.0011, LR: 0.000000\n",
            "Epoch 27/50, Loss: 0.0431, Avg Grad Norm: 0.0097, LR: 0.000000\n",
            "Epoch 28/50, Loss: 0.0435, Avg Grad Norm: 0.0087, LR: 0.000000\n",
            "Epoch 29/50, Loss: 0.0436, Avg Grad Norm: 0.0209, LR: 0.000000\n",
            "Epoch 30/50, Loss: 0.0433, Avg Grad Norm: 0.0027, LR: 0.000000\n",
            "Epoch 31/50, Loss: 0.0435, Avg Grad Norm: 0.0117, LR: 0.000000\n",
            "Epoch 32/50, Loss: 0.0435, Avg Grad Norm: 0.0117, LR: 0.000000\n",
            "Epoch 33/50, Loss: 0.0436, Avg Grad Norm: 0.0179, LR: 0.000000\n",
            "Epoch 34/50, Loss: 0.0434, Avg Grad Norm: 0.0087, LR: 0.000000\n",
            "Epoch 35/50, Loss: 0.0437, Avg Grad Norm: 0.0180, LR: 0.000000\n",
            "Epoch 36/50, Loss: 0.0434, Avg Grad Norm: 0.0055, LR: 0.000000\n",
            "Epoch 37/50, Loss: 0.0434, Avg Grad Norm: 0.0027, LR: 0.000000\n",
            "Epoch 38/50, Loss: 0.0432, Avg Grad Norm: 0.0037, LR: 0.000000\n",
            "Epoch 39/50, Loss: 0.0430, Avg Grad Norm: 0.0130, LR: 0.000000\n",
            "Epoch 40/50, Loss: 0.0436, Avg Grad Norm: 0.0178, LR: 0.000000\n",
            "Epoch 41/50, Loss: 0.0433, Avg Grad Norm: 0.0010, LR: 0.000000\n",
            "Epoch 42/50, Loss: 0.0429, Avg Grad Norm: 0.0160, LR: 0.000000\n",
            "Epoch 43/50, Loss: 0.0438, Avg Grad Norm: 0.0271, LR: 0.000000\n",
            "Epoch 44/50, Loss: 0.0432, Avg Grad Norm: 0.0068, LR: 0.000000\n",
            "Epoch 45/50, Loss: 0.0434, Avg Grad Norm: 0.0057, LR: 0.000000\n",
            "Epoch 46/50, Loss: 0.0433, Avg Grad Norm: 0.0038, LR: 0.000000\n",
            "Epoch 47/50, Loss: 0.0436, Avg Grad Norm: 0.0208, LR: 0.000000\n",
            "Epoch 48/50, Loss: 0.0433, Avg Grad Norm: 0.0038, LR: 0.000000\n",
            "Epoch 49/50, Loss: 0.0433, Avg Grad Norm: 0.0012, LR: 0.000000\n",
            "Epoch 50/50, Loss: 0.0431, Avg Grad Norm: 0.0099, LR: 0.000000\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Create the directory for saving model checkpoints if it doesn't exist\n",
        "os.makedirs('models/Epoch_save', exist_ok=True)\n",
        "\n",
        "# -------------------------------\n",
        "# MODEL TRAINING\n",
        "# -------------------------------\n",
        "train_model(model, optimizer, scheduler, criterion, start_epoch=0, num_epoch=num_epoch, train_loader=train_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}